<!DOCTYPE HTML>
<!--
    Strata by HTML5 UP
    html5up.net | @n33co
    Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
    <head>
              <title>Bloat: origins, monitoring and managing &larr; Travelling SysOp</title>
              <meta http-equiv="content-type" content="text/html; charset=utf-8" />
        <meta name="description" content="Robert M. Wysocki - blog on GNU/Linux and PostgreSQL" />
        <meta name="keywords" content="postgresql linux gnu technology" />
        <!--[if lte IE 8]><script src="/css/ie/html5shiv.js"></script><![endif]-->
        <link rel="stylesheet" href="/css/cookiecuttr.css" />
        <link rel="icon" type="image/x-icon" href="/images/favicon/favicon.ico" />
        <meta name="msapplication-TileColor" content="#ffffff" />
        <meta name="msapplication-TileImage" content="/images/favicon/server---database-144-171423.png" />
        <link rel="apple-touch-icon-precomposed" href="/images/favicon/server---database-152-171423.png" />
        <link rel="apple-touch-icon-precomposed" sizes="152x152" href="/images/favicon/server---database-152-171423.png" />
        <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/images/favicon/server---database-144-171423.png" />
        <link rel="apple-touch-icon-precomposed" sizes="120x120" href="/images/favicon/server---database-120-171423.png" />
        <link rel="apple-touch-icon-precomposed" sizes="114x114" href="/images/favicon/server---database-114-171423.png" />
        <link rel="apple-touch-icon-precomposed" sizes="72x72" href="/images/favicon/server---database-72-171423.png" />
        <link rel="apple-touch-icon-precomposed" href="/images/favicon/server---database-57-171423.png" />
        <link rel="icon" href="/images/favicon/server---database-32-171423.png" sizes="32x32" />
        
        <script src="/meta/0001/01/bundle.js"></script>
        <noscript>
            
            <link rel="stylesheet" href="/meta/0001/01/bundle.css" />
        </noscript>
        <script>
            $(document).ready(function () {
                $.cookieCuttr({
                  cookieAnalyticsMessage: 'This site uses cookies to provide a better experience and collect statistics. Disable them in your web browser if you do not want them.',
                  cookieAcceptButtonText: 'Okay'
                });  
            });
        </script>
       <script type="application/ld+json">
          {
            "@context": "http://schema.org",
            "@type": "Person",
            "name": "Robert M. Wysocki",
            "givenName": "Robert",
            "additionalName": "Mateusz",
            "familyName": "Wysocki",
            "description": "highly experienced GNU/Linux administrator and PostgreSQL DBA",
            "url": "http://rmwpl.github.io",
            "image": "http://rmwpl.github.io/images/por2.jpg",
            "sameAs": [  "http://rmwpl.github.io",  "http://rmwpl.github.io/meta/0001/01/contact.html",  "http://plus.google.com/114199101243673456767",  "http://www.linkedin.com/in/rmwysocki"  ]
          }
        </script>
        <!--[if lte IE 8]><link rel="stylesheet" href="/css/ie/v8.css" /><![endif]-->
    </head>
    <body id="top">

        <!-- Header -->
            <header id="header">
                <a href="http://rmwpl.github.io" class="image avatar"><img src="/images/por2.jpg" alt="" /></a>
                <h1>
                  <strong>Robert M. Wysocki</strong><br />
                  travelling<br />
                  GNU/Linux SysOp<br />
                  <span class="accent">&</span> PostgreSQL DBA<br />
                </h1>
            </header>

        <!-- Main -->
            <div id="main">

                <!-- One -->                    <section id="one">
                        <header class="major">
                            <h2><a href="/postgresql/2016/03/bloat-origins-monitoring-and-managing.html">Bloat: origins, monitoring and managing</a></h2>
                            <h5>
                            
                              <span class="date fa-calendar"> 2016-03-13T14:20:00 GMT &#9474; </span>
                            
                              <span class="disqus fa-comments disqus-comment-comment">&nbsp;
                                <a href="/postgresql/2016/03/bloat-origins-monitoring-and-managing.html#disqus_thread">Comment</a>
                              </span>
                            </h5>
                        </header>
                        <p>PostgreSQL's <a href="http://www.postgresql.org/docs/current/static/mvcc.html">MVCC model</a> provides excellent support for running multiple transactions operating on the same data set. One natural consequence of its design is the existence of so-called "database bloat". Judging by the amount of questions raised in the Internet it is quite a common problem and not many people seem to know, how to properly deal with it. I myself had the same issues and learnt one or two things that might be helpful, so in this article I'd like to shed some light on this notion - why it's there in the first place, how it can affect performance and finally how to monitor and manage it.</p>

<p><em>This article was <a href="https://www.compose.io/articles/postgresql-bloat-origins-monitoring-and-managing/">published</a> by <a href="https://www.compose.io">compose.io</a> as a part of <a href="https://www.compose.io/write-stuff/">Write Stuff</a> programme. Unlike other articles on my blog, this one is licensed under the terms of CC BY-NC-SA 4.0</em></p>

<h1 id="whyitsthere">Why it's there</h1>

<p>In PostgreSQL MVCC - which stands for MultiVersion Concurrency Control - provides each database transaction with a consistent snapshot of the data. (It's important to remember that if no explicit transaction has been opened, each query is a transaction on its own.) This means that changes made in one transaction will become visible to other transaction only after their "origin" transaction successfully commits. Actually, it also depends on other transactions' <a href="http://www.postgresql.org/docs/current/static/transaction-iso.html">isolation level</a>, but to keep this article simple let's not worry about that.</p>

<p>What's really amazing about MVCC is that it manages to deal with quite a complex task like concurrency control using a very simple and clean design. <br />
In order to understand why databases get bloated, it's important to get a bit more familiar with MVCC, so let's have a look.</p>

<pre><code>compose (session 1) =&gt; create table test as select id from generate_series(1,1000) as id;
SELECT 1000
compose (session 1) =&gt; begin;
BEGIN
compose (session 1) =&gt; select txid_current();
 txid_current 
--------------
         1844
(1 row)

compose (session 1) =&gt; select xmin, xmax, id from test limit 5;
 xmin | xmax | id 
------+------+----
 1843 |    0 |  1
 1843 |    0 |  2
 1843 |    0 |  3
 1843 |    0 |  4
 1843 |    0 |  5
(5 rows)
</code></pre>

<p>As you can see, in database session 1 I've created a table called <code>test</code> with only one integer column <code>id</code> and filled it with 1000 rows. Next, I've opened an explicit transaction in which you can see it's identifier and first 5 rows of the test table.</p>

<p>Apart from the table's <code>id</code> column I've instructed the query to print also two system columns: <code>xmin</code> and <code>xmax</code>. The former stores the identifier of transaction that has inserted the row and the latter - the identifier of transaction that has deleted it. In case the row hasn't been deleted it, <code>xmax</code> equals <code>0</code>.</p>

<p>The explicit transaction I've opened has been assigned <code>1844</code> identifier. It is greater-or-equal than <code>1843</code>, so this particular transaction is able to see the rows.</p>

<p>Now please observe what happens in a second database session.</p>

<pre><code>compose (session 2) =&gt; begin;
BEGIN
compose (session 2) =&gt; select txid_current();
 txid_current 
--------------
         1846
(1 row)

compose (session 2) =&gt; select xmin, xmax, id from test limit 2;
 xmin | xmax | id 
------+------+----
 1843 |    0 |  1
 1843 |    0 |  2
(2 rows)
</code></pre>

<p>Nothing surprising here, I hope that's clear.</p>

<p>Let's delete a tuple in the first session (remember that we still have a transaction opened there).</p>

<pre><code>compose (session 1) =&gt; delete from test where id = 1;
DELETE 1
compose (session 1) =&gt; select xmin, xmax, id from test where id = 1;
 xmin | xmax | id 
------+------+----
(0 rows)
</code></pre>

<p>And right after that in session 2:</p>

<pre><code>compose (session 2) =&gt; select xmin, xmax, id from test where id = 1;
 xmin | xmax | id 
------+------+----
 1843 | 1844 |  1
(1 row)
</code></pre>

<p>Although this session's identifier is greater-or-equal than <code>1844</code>, it can still see the deleted row, since the deleting transaction hasn't committed its work yet.</p>

<p>This is really important - even though the row has been deleted, it has not been physically removed from the data page. It can't be removed, since some transactions may still see it in their snapshots as a standard, live row. Same thing goes for updates, since an <code>UPDATE</code> statement is under the hood nothing else, but a <code>DELETE</code> and <code>INSERT</code> combined.</p>

<p>Now it's time to clean up:</p>

<pre><code>compose (session 1) =&gt; commit;
COMMIT
compose (session 2) =&gt; select xmin, xmax, id from test where id = 1;
 xmin | xmax | id 
------+------+----
(0 rows)
</code></pre>

<p>After transaction run in session 1 commits, transaction running in session 2 immediately ceases to see the deleted row.</p>

<pre><code>compose (session 2) =&gt; vacuum verbose test;
INFO:  vacuuming "public.test"
INFO:  "test": removed 1 row versions in 2 pages
INFO:  "test": found 1 removable, 999 nonremovable row versions in 5 out of 5 pages
DETAIL:  0 dead row versions cannot be removed yet.
</code></pre>

<p>As you can see, after all is done, <code>VACUUM</code> takes care of the deleted tuple; it checks that there are no longer any transactions running that can see this row and it removes it (well, it's a bit more complicated, but about that later). Rows like this are called "dead rows" and if not for <code>VACUUM</code>, they would remain in the database's files forever.</p>

<p>This simple exercise demonstrates the process leading to formation of what is known as "database bloat". Under certain circumstances, with <a href="http://www.postgresql.org/docs/current/static/routine-vacuuming.html#AUTOVACUUM">autovacuum daemon</a> not aggressive enough, for heavily-written tables bloat can be a problem that has to be taken care of by the DBA.</p>

<p>The mechanics of MVCC make it obvious why <code>VACUUM</code> exists and the rate of changes in databases nowadays makes a good case for the existence of <code>autovacuum daemon</code>. Although it's common for new PostgreSQL users to be reluctant about setting autovacuum to on (or eager to turn it off, as it's on by default), folks at <code>compose.io</code> did a good job.</p>

<pre><code>compose (session 2) =&gt; select name, setting from pg_settings where name = 'autovacuum';
    name    | setting 
------------+---------
 autovacuum | on
(1 row)
</code></pre>

<h1 id="whatproblemsdoesitcause">What problems does it cause</h1>

<p>No imagine your table has a lot of dead tuples. Each sequential scan going through the table has to make its way to live rows dodging the dead ones. Obviously, it makes it go lot slower. On the other hand each index scan has to follow index pointers to dead rows just to see that they're... well, dead. So again - it could've been faster if not for the bloat. (Yes, indexes can also get bloated, but to keep things simple this article covers only data bloat.)</p>

<p>Another problem is that <code>VACUUM</code> does not actually remove all the dead tuples. It has some limitations. What <code>VACUUM</code> does is simply check, if the tuple is dead and if so, mark the space it utilises as suitable for being reused. So it most cases it actually does not return the disk space to the underlying filesystem. The reason for that is simple - data in PostgreSQL data files is organised in pages, each page having 8kB by default. <code>VACUUM</code> is able to shirk data files only, if the rows deemed dead are at the very end of the very last data page of the entire data file (it's worth mentioning here, that the data file is actually stored in a number of 1GB fragments).</p>

<p>I've seen cases where a large table had its rows updated few times and then was only read from; even though <code>VACUUM</code> has marked the dead rows as suitable for reuse, the data file was twice (or more!) the size of the table (or, to be exact, the sum of sizes of the live rows in the table in question).</p>

<p>What's also quite problematic when reading from heavily bloated tables is the I/O; the more data to go through the more input/output operations are needed. </p>

<p>As you can see, lots of dead rows can cause lots of problems, so it's necessary to at least keep track of the bloat percentage of your tables.</p>

<p>Let's have a look at one simple scenario:</p>

<pre><code>compose (session 1) =&gt; truncate test;
TRUNCATE TABLE
compose (session 1) =&gt; insert into test (id) select * from generate_series(1,10000000);
INSERT 0 10000000
compose (session 1) =&gt; \d+
                  List of relations
 Schema | Name | Type  | Owner |  Size  | Description 
--------+------+-------+-------+--------+-------------
 public | test | table | admin | 346 MB | 
(1 row)

compose (session 1) =&gt; explain (buffers, analyse) select * from test where id = 9999999;
                                              QUERY PLAN                                               
-------------------------------------------------------------------------------------------------------
 Seq Scan on test  (cost=0.00..176992.00 rows=1 width=4) (actual time=855.088..855.089 rows=1 loops=1)
   Filter: (id = 9999999)
   Rows Removed by Filter: 9999999
   Buffers: shared hit=16202 read=28046
 Planning time: 0.043 ms
 Execution time: 855.120 ms
(6 rows)
</code></pre>

<p>I've truncated our test table and inserted 10,000,000 new rows. Then, I've selected one of them. It took some time since there's no index and PostgreSQL had to go through all the data pages to find the requested row. What's most interesting in this exercise are the hit and read buffer counters; they show, how many rows have been found in shared buffers (hit) and how many have been read from disk (read).</p>

<p>Now, let's create some bloat!</p>

<pre><code>compose (session 1) =&gt; delete from test where id &gt; 1000 and id &lt; 9000000;
DELETE 8998999

compose (session 1) =&gt; vacuum analyse verbose test;
INFO:  vacuuming "public.test"
INFO:  "test": removed 8998999 row versions in 39820 pages
INFO:  "test": found 8998999 removable, 1001001 nonremovable row versions in 44248 out of 44248 pages
DETAIL:  0 dead row versions cannot be removed yet.
There were 0 unused item pointers.
0 pages are entirely empty.
CPU 0.50s/1.05u sec elapsed 2.11 sec.
INFO:  analyzing "public.test"
INFO:  "test": scanned 30000 of 44248 pages, containing 671315 live rows and 0 dead rows; 30000 rows in sample, 993641 estimated total rows
VACUUM

compose (session 1) =&gt; \d+
                  List of relations
 Schema | Name | Type  | Owner |  Size  | Description 
--------+------+-------+-------+--------+-------------
 public | test | table | admin | 346 MB | 
(1 row)
</code></pre>

<p>As you can see, I've deleted most of the rows and then run a <code>VACUUM</code> which reported that there are no pages entirely empty, so no pages have been removed hence the table size stays the same. The disk space occupied by the deleted rows have now been marked as available for reuse, but it is still part of this table's data file.</p>

<p>Let's see, what happens, if I'll run the same <code>SELECT</code> now.</p>

<pre><code>compose (session 1) =&gt; explain (buffers, analyse) select * from test where id = 9999999;
                                              QUERY PLAN                                              
------------------------------------------------------------------------------------------------------
 Seq Scan on test  (cost=0.00..56668.51 rows=1 width=4) (actual time=178.370..178.372 rows=1 loops=1)
   Filter: (id = 9999999)
   Rows Removed by Filter: 1001000
   Buffers: shared hit=16219 read=28029
 Planning time: 0.053 ms
 Execution time: 178.409 ms
(6 rows)
</code></pre>

<p>It it faster, then the previous one, but notice that the hit and read buffer counters are more or less the same, as during the previous run. What this mean is that this time PostgreSQL has read the buffers not directly from disk, but from OS cache. Basically the amount of work PostgreSQL had to do now was pretty much the same, as previously.</p>

<p>What can we do about it?</p>

<pre><code>compose (session 1) =&gt; vacuum full analyse verbose test;
INFO:  vacuuming "public.test"
INFO:  "test": found 0 removable, 1001001 nonremovable row versions in 44248 pages
DETAIL:  0 dead row versions cannot be removed yet.
CPU 0.16s/0.36u sec elapsed 0.58 sec.
INFO:  analyzing "public.test"
INFO:  "test": scanned 4430 of 4430 pages, containing 1001001 live rows and 0 dead rows; 30000 rows in sample, 1001001 estimated total rows
VACUUM
compose (session 1) =&gt; \d+
                  List of relations
 Schema | Name | Type  | Owner | Size  | Description 
--------+------+-------+-------+-------+-------------
 public | test | table | admin | 35 MB | 
(1 row)
</code></pre>

<p><code>VACUUM FULL</code> is one of the ways of removing bloat. It essentially rewrites the whole table (holding an <code>AccessExclusiveLock</code> while doing it). Much has been said about why not to use <code>VACUUM FULL</code> if there are other ways of dealing with bloat. But it's perfect for our simple exercise.</p>

<p>Thanks to a full rewrite, table size went down from ~350MB to 35MB.</p>

<p>Let's select the same row now.</p>

<pre><code>compose (session 1) =&gt; explain (buffers, analyse) select * from test where id = 9999999;
                                             QUERY PLAN                                             
----------------------------------------------------------------------------------------------------
 Seq Scan on test  (cost=0.00..16942.51 rows=1 width=4) (actual time=86.967..86.969 rows=1 loops=1)
   Filter: (id = 9999999)
   Rows Removed by Filter: 1001000
   Buffers: shared hit=32 read=4398
 Planning time: 0.125 ms
 Execution time: 87.005 ms
(6 rows)
</code></pre>

<p>Thanks to removing the bloat we created, PostgreSQL has now much less data to go through and is able to satisfy our query much faster.</p>

<h1 id="howtomonitorit">How to monitor it</h1>

<p>Monitoring database bloat is pretty simple; the useful <a href="https://github.com/bucardo/check_postgres">check_postgres</a> script can do that for you. Apart from it, there are <a href="http://www.databasesoup.com/2014/10/new-table-bloat-query.html">few</a> <a href="http://blog.ioguix.net/postgresql/2014/09/10/Bloat-estimation-for-tables.html">various</a> <a href="https://wiki.postgresql.org/wiki/Show_database_bloat">queries</a>. It is also possible to <a href="http://www.cybertec.at/2013/08/detecting-table-bloat/">use</a> the <a href="http://www.postgresql.org/docs/current/static/pgstattuple.html">pgstattuple</a> module.</p>

<p>My personal favourite is a <a href="https://github.com/ioguix/pgsql-bloat-estimation/blob/master/table/table_bloat-82-84.sql">query</a> by <a href="https://github.com/ioguix">ioguix</a>.</p>

<p>Let me show you its output on my test database.</p>

<pre><code>compose (session 1) =&gt; truncate test;
TRUNCATE TABLE
compose (session 1) =&gt; insert into test (id) select * from generate_series(1,10000000);
INSERT 0 10000000
compose (session 1) =&gt; delete from test where id &gt; 1000 and id &lt; 9000000;
DELETE 8998999
compose (session 1) =&gt; \e
 current_database | schemaname | tblname | real_size | extra_size |   extra_ratio    | fillfactor | bloat_size |   bloat_ratio    | is_na 
------------------+------------+---------+-----------+------------+------------------+------------+------------+------------------+-------
 compose          | public     | test    | 362479616 |  324009984 | 89.3870909419635 |        100 |  324009984 | 89.3870909419635 | f
(1 row)
</code></pre>

<h1 id="howtodealwithit">How to deal with it</h1>

<p>As already mentioned, the traditional way of dealing with bloat is using <code>VACUUM FULL</code>. This isn't really advised in production environments mainly because heavy locking - this utility query has to acquire and hold throughout its duration the strongest existing lock - <code>AccessExclusiveLock</code>. It basically takes the whole table out of any operation until it finishes.</p>

<p>But fortunately there are better ways of removing bloat nowadays. Basically, it can be done in one of two ways.</p>

<p>Firstly, we can rewrite the table as <code>VACUUM FULL</code> does, but in a more gracious way. The approach <a href="https://github.com/reorg/pg_repack">pg_repack</a> takes is simple, yet very effective - let's create an identical, but empty table, place some triggers on the original one and copy its contents to the new one. Triggers take care of synchronising all the changes (including new rows) to the intermediate table while the process is running and after it's done, <code>pg_repack</code> recreates all the indexes and swaps the two tables. This way the original table isn't totally locked and there's only a brief outage when the tables have to be swapped. One downside though is that <code>pg_repack</code> requires extra disk space for the intermediate table.</p>

<p>Secondly, we can force table reorganisation to reuse the space marked as available by <code>VACUUM</code> runs. The great <a href="https://github.com/grayhemp/pgtoolkit/blob/master/bin/pgcompact">pgcompact</a> script does that using <a href="http://www.depesz.com/2013/06/21/bloat-removal-by-tuples-moving/">the idea</a> presented by <a href="http://www.depesz.com/2013/10/15/bloat-removal-without-table-swapping/">depesz</a> in two blog posts. Basically, knowing which pages in our data file have gaps we can try and execute so-called "empty updates" forcing PostgreSQL to create new row versions and to place them in the gaps we want to fill. Head to depesz's posts for detailed description. One other thing worth mentioning is that <code>pgcompact</code> is smart enough to prevent any triggers you might have from firing by setting <code>session_replication_role</code> to <code>replica</code>. Neat!</p>

<p>Let's have a look at clearing bloat (using <code>pgcompact</code>) on my test database hosted at <a href="http://compose.io">compose.io</a>.</p>

<pre><code>$ ./pgcompact -v info -h aws-eu-west-1-portal.2.dblayer.com -p 10392 -U admin -W my_secret_password -d compose -t test
Sat Mar 12 17:05:10 2016 ERROR A database error occurred, exiting:
DatabaseChooserError Can not find an adapter amongst supported: 
DatabaseError Can not execute command: 
SET lc_messages TO 'C'; SET session_replication_role TO replica; SET statement_timeout TO '0'; SET synchronous_commit TO off;
ERROR:  permission denied to set parameter "lc_messages"
DatabaseError Can not execute command: 
SET lc_messages TO 'C'; SET session_replication_role TO replica; SET statement_timeout TO '0'; SET synchronous_commit TO off;
ERROR:  permission denied to set parameter "lc_messages"

DatabaseError Can not executie command: 
 SET lc_messages TO 'C'; SET session_replication_role TO replica; SET statement_timeout TO '0'; SET synchronous_commit TO off; SELECT 1;
 ERROR:  permission denied to set parameter "lc_messages"
ERROR:  permission denied to set parameter "session_replication_role"
</code></pre>

<p>Oh darn! Looks like we need superuser privileges to set some of the parameters.</p>

<p>Let's try <code>pg_repack</code>.</p>

<pre><code>$ ./pg_repack -E info -h aws-eu-west-1-portal.2.dblayer.com -p 10392 -U admin -d compose -t test
ERROR: pg_repack failed with error: You must be a superuser to use pg_repack
</code></pre>

<p>Ouch! The thing is that <code>pg_repack</code> modifies system catalogues directly in order to swap the two tables. This can only be done by superuser.</p>

<p>Fortunately <a href="http://www.depesz.com/2013/06/21/bloat-removal-by-tuples-moving/">one of depesz's posts</a> covers going through the process manually and there's no reason it shouldn't work.</p>

<h1 id="furtherreading">Further reading</h1>

<p>Some resources worth taking a look at:</p>

<ul>
<li><p><a href="http://momjian.us/main/writings/pgsql/mvcc.pdf">Bruce Momjian's presentation on MVCC</a></p></li>
<li><p><a href="http://www.depesz.com/?s=bloat">Hubert Lubaczewski's posts on bloat in PostgreSQL</a></p></li>
<li><p><a href="https://wiki.postgresql.org/wiki/Hint_Bits">PostgreSQL wiki page on MVCC hint bits</a></p></li>
<li><p><a href="http://www.postgresql.org/docs/current/static/routine-vacuuming.html#AUTOVACUUM">Autovacuum deamon's official documentation</a></p></li>
</ul>

                        <hr/>
                        <p>If you like this article please consider sharing it with your friends.</p>
                        <ul class="actions">
                            <li><div class="addthis_sharing_toolbox"></div></li>
                            <li>
                                <div id="disqus_thread"></div>
                                <script type="text/javascript">
                                    /* * * CONFIGURATION VARIABLES * * */
                                    var disqus_shortname = 'rmwpl';
                                    
                                    /* * * DON'T EDIT BELOW THIS LINE * * */
                                    (function() {
                                        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                                        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                                        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
                                    })();
                                </script>
                                <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
                            </li>
                        </ul>
                    </section>
            </div>

        <!-- Footer -->
            <footer id="footer">
                <ul class="icons">
                    <li><a href="http://www.linkedin.com/in/rmwysocki" class="icon fa-linkedin-square"><span class="label">LinkedIn</span></a></li>
                    <li><a href="http://plus.google.com/114199101243673456767" class="icon fa-google-plus-square"><span class="label">Google+</span></a></li>
                    <li><a href="http://rmwpl.github.io/meta/0001/01/contact.html" class="icon fa-envelope-o"><span class="label">Email</span></a></li>
                </ul>
                <ul class="copyright">
                    <li>Layout: <a href="http://html5up.net">HTML5 UP</a></li>
                    <br />
                    <li class="fa fa-gear"> Built with: <a href="http://vanilladraft.com/dapper/">Dapper</a> and <a href="http://www.perl.org">perl</a></li>
                    <li>
                      <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">
                        <img alt="Creative Commons License" style="border-width:0; text-decoration:none;" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" />
                        Copyleft: Robert M. Wysocki</a>
                    </li>
                </ul>
            </footer>

        <script type="text/javascript" src="http://s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5557bc9c7e0bede0" async="async"></script>
        <script type="text/javascript">
            /* * * CONFIGURATION VARIABLES * * */
            var disqus_shortname = 'rmwpl';
            
            /* * * DON'T EDIT BELOW THIS LINE * * */
            (function () {
                var s = document.createElement('script'); s.async = true;
                s.type = 'text/javascript';
                s.src = '//' + disqus_shortname + '.disqus.com/count.js';
                (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
            }());
        </script>
        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-63081003-1', 'auto');
          ga('send', 'pageview');

        </script>
    </body>
</html>
